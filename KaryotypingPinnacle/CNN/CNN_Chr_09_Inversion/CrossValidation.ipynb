{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "slIdjqTJhM8B"
   },
   "source": [
    "# Facilitated Machine Learning Models for Karyotyping in the Patients with Chromosomal Abnormalities: Retrospective Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBT9lgVVhM8D"
   },
   "source": [
    "- **Chuan Yang**, MD, PhD Student\n",
    "- Mentor: **Yanyan Zhao**, MD, PhD\n",
    "- Shengjing Hospital of China Medical University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrnbxOZChM8E"
   },
   "source": [
    "# 0. Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKjA8v4hhM8E"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow import keras\n",
    "\n",
    "from os import walk\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chr 09 vs Chr 09 Inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2KFSsnYohM8H"
   },
   "source": [
    "# 1. Samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. File Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Filename Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YcabYh2hM8I"
   },
   "outputs": [],
   "source": [
    "# pathBase = 'C:\\\\Users\\\\Chuan\\\\OneDrive\\\\Dowrun\\\\Database\\\\PhD\\\\KaryoTypes\\\\Arrangement\\\\'\n",
    "# pathBase = 'I:\\\\Chuan\\\\Documents\\\\MyData\\\\PhD\\\\Karyotype\\\\Arrangement\\\\'\n",
    "# pathBase = 'D:\\\\Users\\\\Chuan\\\\Documents\\\\Database\\\\Karyotypes\\\\Arrangement\\\\'\n",
    "# ///////////////////////////////////////////////\n",
    "# Merged Database\n",
    "pathBase = 'D:\\\\Users\\\\Chuan\\\\Documents\\\\Database\\\\Karyotypes\\\\Arrangement_Merged\\\\'\n",
    "\n",
    "#pathBase = 'I:\\\\Chuan\\\\Documents\\\\MyData\\\\PhD\\\\Karyotype\\\\Arrangement_Merged\\\\'\n",
    "\n",
    "theWhole = {}\n",
    "\n",
    "\n",
    "f = []\n",
    "f_09 = []\n",
    "mypath_09 = pathBase + 'chr_09'\n",
    "for (dirpath, dirnames, filenames) in walk(mypath_09):\n",
    "    f.extend(filenames)\n",
    "for l in f:\n",
    "    f_09.append(mypath_09 + '\\\\' + l)    \n",
    "    \n",
    "# ///////// Abnormal ones //////////////\n",
    "\n",
    "f = []\n",
    "f_9_inversion = []\n",
    "mypath_9_inversion = pathBase + 'chr_9_inversion'\n",
    "for (dirpath, dirnames, filenames) in walk(mypath_9_inversion):\n",
    "    f.extend(filenames)\n",
    "for l in f:\n",
    "    f_9_inversion.append(mypath_9_inversion + '\\\\' + l)  \n",
    "    \n",
    "theWhole['chr_09'] = f_09    \n",
    "theWhole['chr_9_inversion'] = f_9_inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(theWhole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(theWhole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theWhole.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's say split every class into 7 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(7, True, 1)\n",
    "\n",
    "# 分成7个子集，每次6个子集用于训练，1个子集用于测试\n",
    "# 每种分法进行一次训练和测试的迭代，总共8次迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Assignment of filename which has been splitted randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_name = {}\n",
    "X_test_name = {}\n",
    "\n",
    "file_k_fold = open('KFold.txt', 'w')\n",
    "\n",
    "# 赋值两个接收文件路径的dictionary，其第一个key值为染色体或异常核型的名称，第二个为迭代的序号\n",
    "\n",
    "for chrNo in theWhole.keys():\n",
    "\n",
    "    X_train_name[chrNo] = {}\n",
    "    X_test_name[chrNo] = {}\n",
    "    \n",
    "    # chrNo为染色体号或异常核型号，在此内部再定义迭代次数\n",
    "\n",
    "    split_method_number = 0\n",
    "    \n",
    "    # 赋值每次split分法的序号值\n",
    "\n",
    "    generator_kFold = kfold.split(theWhole[chrNo])\n",
    "    \n",
    "    # 依染色体或异常核型的类型进行split，因为每个类别的样本量不均衡，以每个类别进行split\n",
    "    # 赋值一个generator对象，以下对generator进行迭代。\n",
    "    \n",
    "    print('Chromosome/Abnormality: ', chrNo)\n",
    "    file_k_fold.write('Chromosome/Abnormality: %s\\n' % chrNo)\n",
    "\n",
    "    for train, test in generator_kFold:    \n",
    "        \n",
    "        # 循环产生train和test集\n",
    "\n",
    "        print('Split Method No. ', split_method_number)\n",
    "        file_k_fold.write('Split Method No. %s\\n' % split_method_number)\n",
    "\n",
    "        print('Train: ', train, 'Test: ', test, '\\n')\n",
    "        file_k_fold.write('Train: %s\\n' % train)\n",
    "        file_k_fold.write('Test: %s\\n\\n' % test)\n",
    "\n",
    "        # train和test的值是7个split分法的每个分法的list\n",
    "\n",
    "        X_train_name[chrNo][split_method_number] = []\n",
    "        X_test_name[chrNo][split_method_number] = []\n",
    "\n",
    "\n",
    "        for split_method_train in train:\n",
    "            \n",
    "            # train 为训练集list中的序号值\n",
    "\n",
    "            # split_method_train的值是每个split方法，其值为图像序号\n",
    "            \n",
    "            X_train_name[chrNo][split_method_number].append(theWhole[chrNo][split_method_train])\n",
    "            \n",
    "            # 将训练集那个序号的图像的文件path和文件名赋值给X_train_name这个二维dictionary\n",
    "\n",
    "\n",
    "        for split_method_test in test:\n",
    "\n",
    "            # split_method_test的值是每个split方法，其值为图像序号\n",
    "            \n",
    "            X_test_name[chrNo][split_method_number].append(theWhole[chrNo][split_method_test])\n",
    "            \n",
    "            # 同样将测试集的路径和文件名赋值给X_test_name\n",
    "\n",
    "        split_method_number = split_method_number + 1\n",
    "        \n",
    "        # Split分法序号自加\n",
    "    file_k_fold.write('\\n')\n",
    "        \n",
    "file_k_fold.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Filenames of Train and Test to a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_X_train_name = open('data_X_train_name.json', 'w')\n",
    "json.dump(X_train_name, file_X_train_name)\n",
    "file_X_train_name.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_X_test_name = open('data_X_test_name.json', 'w')\n",
    "json.dump(X_test_name, file_X_test_name)\n",
    "file_X_test_name.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_theWhole = open('data_theWhole.json', 'w')\n",
    "json.dump(theWhole, file_theWhole)\n",
    "file_theWhole.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the File to Acquire the Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data_X_train_name.json') as json_file:\n",
    "    X_train_name = json.load(json_file)\n",
    "X_train_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_X_test_name.json') as json_file:\n",
    "    X_test_name = json.load(json_file)\n",
    "X_test_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_theWhole.json') as json_file:\n",
    "    theWhole = json.load(json_file)\n",
    "theWhole"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "KaryoChimneyRock_FullyConnectedNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
